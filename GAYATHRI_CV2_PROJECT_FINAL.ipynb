{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QH63kSvGLqm"
   },
   "source": [
    "**Part: A TO BUILD A FACE DETECTION MODEL**\n",
    "\n",
    "DOMAIN: Entertainment\n",
    "\n",
    "• CONTEXT: Company X owns a movie application and repository which caters movie streaming to millions of users who on subscription basis.\n",
    "Company wants to automate the process of cast and crew information in each scene from a movie such that when a user pauses on the movie\n",
    "and clicks on cast information button, the app will show details of the actor in the scene. Company has an in-house computer vision and\n",
    "multimedia experts who need to detect faces from screen shots from the movie scene.The data labelling is already done. Since there higher time complexity is involved in the\n",
    "\n",
    "• DATA DESCRIPTION: The dataset comprises of images and its mask for corresponding human face.\n",
    "\n",
    "• PROJECT OBJECTIVE: To build a face detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "xgK5DpGWGgg2",
    "outputId": "7e8fc7f7-0143-4775-b85c-7063d79cdc97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'2.9.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdFrQ-ZAGgvV"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "# Ignore the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f-3uyEakGgyQ",
    "outputId": "5a791809-6c91-4bc5-f05a-ca2f411da804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/computervision/CV_PROJECT2\n"
     ]
    }
   ],
   "source": [
    "cd '/content/drive/MyDrive/computervision/CV_PROJECT2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omzAutVPHc0I"
   },
   "source": [
    "**Importing the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Xp8N-e-hGg07"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling2D, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras import backend as K\n",
    "from PIL import Image\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "__qtugXfGg3s"
   },
   "outputs": [],
   "source": [
    "# Loading the images file\n",
    "\n",
    "data = np.load('images.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UvTP_pWGg6T"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3zd4YoxIH-7"
   },
   "source": [
    "The file contains 409 images and labels. Let's view few images and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g7vj1D_rID1M"
   },
   "outputs": [],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qfsu8tPxID_z"
   },
   "outputs": [],
   "source": [
    "data[408][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TolStZ0_IM95"
   },
   "source": [
    "Viewing few random images and labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THbyoY9eIEJu"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "cv2_imshow(data[23][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUQCSTFJIEMt"
   },
   "outputs": [],
   "source": [
    "fi,ax = plt.subplots(10,3,figsize=(20,30))\n",
    "row = 0\n",
    "col = 0\n",
    "index = 0\n",
    "for i in range(30):\n",
    "  ax[row][col].imshow(data[index][0], interpolation='nearest')\n",
    "  index = index + 12\n",
    "  col = col + 1\n",
    "  if col > 2:\n",
    "    row = row + 1\n",
    "    col = 0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMKykjK6ISEu"
   },
   "source": [
    "Creating features (images) and labels (mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9xREfPyIEPm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "\n",
    "HEIGHT_CELLS = 28\n",
    "WIDTH_CELLS = 28\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "masks = np.zeros((int(data.shape[0]), IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "X = np.zeros((int(data.shape[0]),IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "\n",
    "for index in range(data.shape[0]):\n",
    "  img = data[index][0]\n",
    "  img = cv2.resize(img, dsize=(IMAGE_HEIGHT, IMAGE_WIDTH), interpolation=cv2.INTER_CUBIC)\n",
    "  # assign all pixels in the first 3 channels only to the image, i.e., discard the alpha channel\n",
    "  try:\n",
    "    img = img[:,:,:3]\n",
    "  except:\n",
    "    print(f\"Exception {index} Grayscale image with shape {img.shape}\")\n",
    "    # convert the grayscale image to color so that the number of channels are standardized to 3\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    continue\n",
    "  X[index] = preprocess_input(np.array(img, dtype=np.float32))\n",
    "  # Loop through the face co-ordinates and create mask out of it.\n",
    "  for i in data[index][1]:\n",
    "    x1 = int(i['points'][0]['x'] * IMAGE_WIDTH)\n",
    "    x2 = int(i['points'][1]['x'] * IMAGE_WIDTH)\n",
    "    y1 = int(i['points'][0]['y'] * IMAGE_HEIGHT)\n",
    "    y2 = int(i['points'][1]['y'] * IMAGE_HEIGHT)\n",
    "    # set all pixels within the mask co-ordinates to 1.\n",
    "    masks[index][y1:y2, x1:x2] = 1\n",
    "print(f\"### Shape of X is '{X.shape}' and the shape of mask is '{masks.shape}' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62vHZJrlIYDe"
   },
   "source": [
    "Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhOsRXdtIESZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, masks, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.2)\n",
    "\n",
    "print(f\"Shape of X_train is '{X_train.shape}' and the shape of y_train is '{y_train.shape}'\")\n",
    "print(f\"Shape of X_val is '{X_val.shape}' and the shape of y_val is '{y_val.shape}'\")\n",
    "print(f\"Shape of X_test is '{X_test.shape}' and the shape of y_test is '{y_test.shape}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwat5IJvIceO"
   },
   "source": [
    "Visualizing X_train and y_train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIYiPdbJIb48"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "a = fig.add_subplot(1, 4, 1)\n",
    "imgplot = plt.imshow(X_train[0])\n",
    "\n",
    "a = fig.add_subplot(1, 4, 2)\n",
    "imgplot = plt.imshow(X_train[10])\n",
    "imgplot.set_clim(0.0, 0.7)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 3)\n",
    "imgplot = plt.imshow(X_train[20])\n",
    "imgplot.set_clim(0.0, 1.4)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 4)\n",
    "imgplot = plt.imshow(X_train[30])\n",
    "imgplot.set_clim(0.0, 2.1)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "a = fig.add_subplot(1, 4, 1)\n",
    "imgplot = plt.imshow(y_train[0])\n",
    "\n",
    "a = fig.add_subplot(1, 4, 2)\n",
    "imgplot = plt.imshow(y_train[10])\n",
    "imgplot.set_clim(0.0, 0.7)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 3)\n",
    "imgplot = plt.imshow(y_train[20])\n",
    "imgplot.set_clim(0.0, 1.4)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 4)\n",
    "imgplot = plt.imshow(y_train[30])\n",
    "imgplot.set_clim(0.0, 1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIk8Zg5iIghm"
   },
   "source": [
    "Creating a Mask Detection Model using U-net with MobileNet Transfer Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj62a6ExIEVQ"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "EPOCHS = 30\n",
    "BATCH = 8\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLTF6ZX1IEX-"
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    inputs = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\n",
    "    \n",
    "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=0.35)\n",
    "    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
    "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    \n",
    "    f = [16, 32, 48, 64]\n",
    "    x = encoder_output\n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = UpSampling2D((2, 2))(x)\n",
    "        x = Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        \n",
    "    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n",
    "    x = Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgaIAdXAIEbD"
   },
   "outputs": [],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Gr93836InZF"
   },
   "source": [
    "Designing Dice Coefficient and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_dZ7puXGg8n"
   },
   "outputs": [],
   "source": [
    "smooth = 1e-15\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FWU7xlfIq5W"
   },
   "source": [
    "Compliling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iWPUr3DIrl3"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "metrics = [dice_coef, Recall(), Precision()]\n",
    "model.compile(loss=dice_loss, optimizer=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41HQmhYaIuPD"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IcCvqGiGM0I"
   },
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxGoT0ZRIx46"
   },
   "outputs": [],
   "source": [
    "train_steps = len(X_train)//BATCH\n",
    "valid_steps = len(X_val)//BATCH\n",
    "\n",
    "if len(X_train) % BATCH != 0:\n",
    "    train_steps += 1\n",
    "if len(X_val) % BATCH != 0:\n",
    "    valid_steps += 1\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6W9XP39I1jU"
   },
   "source": [
    "Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u96fbCc2I0zc"
   },
   "outputs": [],
   "source": [
    "test_steps = (len(X_test)//BATCH)\n",
    "if len(X_test) % BATCH != 0:\n",
    "    test_steps += 1\n",
    "\n",
    "model.evaluate(X_test, y_test, steps=test_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBGmqpemI09S"
   },
   "source": [
    "The model has precision and recall of 55% and 76.13% respectively. The loss is 50% and dice coefficient is 49.8%.\n",
    "\n",
    "Predicting an image that was not used for training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XT3mUj0ZI7IW"
   },
   "outputs": [],
   "source": [
    "filename = '/content/drive/MyDrive/computervision/CV_PROJECT2/predict_image3.jfif'\n",
    "unscaled = cv2.imread(filename)\n",
    "image = cv2.resize(unscaled, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "feat_scaled = preprocess_input(np.array(image, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83487a1eI8-k"
   },
   "outputs": [],
   "source": [
    "feat_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23GOMVUbI-8Y"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(np.array([feat_scaled]))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4kZGiDMI-_j"
   },
   "outputs": [],
   "source": [
    "pred_mask = cv2.resize((1.0*(y_pred[0] > 0.5)), (IMAGE_WIDTH,IMAGE_HEIGHT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws7amcvxJMdM"
   },
   "source": [
    "Viewing the predicted image and its face-detected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18iFceQyI_C9"
   },
   "outputs": [],
   "source": [
    "plt.imshow(feat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKgXXy8BI_GQ"
   },
   "outputs": [],
   "source": [
    "plt.imshow(pred_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn4iQpxHJSo4"
   },
   "source": [
    "The model was able to detect two faces in the image correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnO5MzhnJXKU"
   },
   "source": [
    "**CNN - Object Detection - Part 2**\n",
    "\n",
    "DOMAIN: Entertainment\n",
    "\n",
    "• CONTEXT: Company X owns a movie application and repository which caters movie streaming to millions of users who on subscription\n",
    "basis. Company wants to automate the process of cast and crew information in each scene from a movie such that when a user pauses on\n",
    "the movie and clicks on cast information button, the app will show details of the actor in the scene. Company has an in-house computer\n",
    "vision and multimedia experts who need to detect faces from screen shots from the movie scene.The data labelling is already done. \n",
    "• DATA DESCRIPTION: The dataset comprises of face images.\n",
    "\n",
    "• PROJECT OBJECTIVE: To create an image dataset to be used by AI team build an image classifier data. Profile images of people are given"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDlpGxADJxai"
   },
   "source": [
    "**Importing the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MCV66XCI_JT"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "from IPython.display import Image, display, Markdown, clear_output\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nA4eXQT7I_MU"
   },
   "outputs": [],
   "source": [
    "project_path = '/content/drive/MyDrive/computervision/CV_PROJECT2/'\n",
    "image_files = 'training_images-20211126T092819Z-001.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgcL58l-J3ot"
   },
   "source": [
    "Unzipping the zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "su42PKCtI_PL"
   },
   "outputs": [],
   "source": [
    "images_zip_path = os.path.join(project_path, image_files)\n",
    "\n",
    "with ZipFile(images_zip_path, 'r') as z:\n",
    "  z.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TNCbOAUJ7UK"
   },
   "source": [
    "Getting the unzipped location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V28jxWmbI_Sf"
   },
   "outputs": [],
   "source": [
    "## Get the Unzipped Location in the drive\n",
    "\n",
    "zip_dir_loc = z.filelist[0].filename.split(\"/\")[0] \n",
    "zip_dir_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3jTlafnJ_CA"
   },
   "source": [
    "Viewing the names of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Wjrj5YMKB6m"
   },
   "outputs": [],
   "source": [
    "raw_img_file_names = [os.path.join(zip_dir_loc,i) for i in os.listdir(zip_dir_loc)]\n",
    "raw_img_file_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D471lrWfKDs-"
   },
   "source": [
    "Reading the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElHbbg7sKERx"
   },
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for imgs in tqdm(raw_img_file_names):\n",
    "  tst_img = cv2.imread(imgs)\n",
    "  img_list.append(tst_img)\n",
    "img_list = np.array(img_list)\n",
    "display(Markdown(f\"#### {img_list.shape}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZslgSe9RKJGi"
   },
   "source": [
    "There are 1091 images with 600 X 600 dimensions, and 3 channels (RGB) which means that the images are colored and not grayscale.\n",
    "\n",
    "**Viewing random images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2_m9pk7KF8i"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "for i in img_list[:5,]:\n",
    "  cv2_imshow(cv2.resize(i,(224,224)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWM_ZJVUKQog"
   },
   "source": [
    "Defining a function to create bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNpjChaJKQE6"
   },
   "outputs": [],
   "source": [
    "def test_bb(df,fname,title=\"\"):\n",
    "\n",
    "  tst_img = cv2.imread(fname)\n",
    "  temp_df = df[df['Image_Name'] == fname]\n",
    "  rect_img = []\n",
    "  for rows in temp_df.index:\n",
    "    x = df['x'][rows]\n",
    "    y = df['y'][rows]\n",
    "    w = df['w'][rows]\n",
    "    h = df['h'][rows]\n",
    "    cv2.rectangle(tst_img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    cv2.putText(tst_img, title, (int((x+w)*0.75),y-3),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255))\n",
    "  cv2_imshow(tst_img)\n",
    "  \n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iAGSoSSKURF"
   },
   "source": [
    "Defining a function to read images and resize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KfD4UoiKVAs"
   },
   "outputs": [],
   "source": [
    "def show_face(img_list,scale=1.0):\n",
    "\n",
    "  for imgs in img_list:\n",
    "    img = cv2.imread(imgs)\n",
    "    img_w  = int(img.shape[1]*scale)\n",
    "    img_h = int(img.shape[0]*scale)\n",
    "    img = cv2.resize(img,(img_w,img_h))\n",
    "    display(Markdown(f\"#### {imgs}\"))\n",
    "    cv2_imshow(img)\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh2Vo3U1KZWD"
   },
   "source": [
    "Downloading the HAAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XGTgqpjKXn7"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJlrVoswKYXR"
   },
   "outputs": [],
   "source": [
    "haar_img_box_df = pd.DataFrame(columns=['x','y','w','h','Total_Faces','Image_Name'])\n",
    "haar_img_box_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXV8UBotKf9B"
   },
   "source": [
    "Detecting Faces using HAAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zs3g0ypKYdK"
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "test_img = cv2.imread(raw_img_file_names[1])\n",
    "grey = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "# Detect faces\n",
    "faces = face_cascade.detectMultiScale(grey,1.1,4)\n",
    "# Draw rectangle around the faces\n",
    "for (x, y, w, h) in faces:\n",
    "  cv2.rectangle(test_img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "  cv2.putText(test_img, \"HaarCascadeClassifier\", (int((x+w)*0.75),y-3),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255))\n",
    "\n",
    "# Display the output\n",
    "display(Markdown(f\"### Bounding Box parameters are `x`:{x}, `y`:{y}, `width`:{w}, `height`:{h}\"))\n",
    "cv2_imshow(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6HYBZ6VKlXw"
   },
   "source": [
    "Detecting faces for all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKgNMlwcKYge"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "haar_undetected_images = []\n",
    "haar_detected_images = []\n",
    "\n",
    "for imgs, fnames in tqdm(zip(img_list,raw_img_file_names)):\n",
    "  gray = cv2.cvtColor(imgs,cv2.COLOR_BGR2GRAY)\n",
    "  faces = face_cascade.detectMultiScale(gray,1.1,4)\n",
    "  if len(faces) == 0:\n",
    "    haar_undetected_images.append(fnames)\n",
    "    temp_dict = {'x':0, \n",
    "                 'y':0, \n",
    "                 'w':-1,\n",
    "                 'h':-1, \n",
    "                 'Total_Faces':0,\n",
    "                 'Image_Name':fnames} \n",
    "  else:\n",
    "    haar_detected_images.append(fnames)\n",
    "    for (x,y,w,h) in faces:\n",
    "      temp_dict = {'x':x, \n",
    "                  'y':y, \n",
    "                  'w':w,\n",
    "                  'h':h, \n",
    "                  'Total_Faces':len(faces),\n",
    "                  'Image_Name':fnames} \n",
    "      haar_img_box_df = haar_img_box_df.append(temp_dict,ignore_index=True)\n",
    "display(Markdown(f\"#### Detected faces for {len(haar_detected_images)} images\"))\n",
    "display(Markdown(f\"#### Failed to detect faces for {len(haar_undetected_images)} images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPEHFXn1KYjT"
   },
   "outputs": [],
   "source": [
    "haar_img_box_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynjaUkS2KtHg"
   },
   "source": [
    "The HAAR was able to detect only 1291 images with one or more faces correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xw_K2XhnKYmY"
   },
   "outputs": [],
   "source": [
    "haar_img_box_df[haar_img_box_df['Total_Faces'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyELi82PKxLz"
   },
   "source": [
    "There are 199 images which have more than one face.\n",
    "\n",
    "**Viewing samples of correctly and incorrectly detected faces using the HAAR Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcbdHLsQKYqn"
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"### (1) Correctly detected 1 face\"))\n",
    "test_bb(haar_img_box_df,\"training_images/real_00115.jpg\",title=\"Haar\")\n",
    "display(Markdown(\"### (2) Incorrectly detected multiple faces\"))\n",
    "test_bb(haar_img_box_df,\"training_images/real_00730.jpg\",title=\"Haar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86sX29TQK8KA"
   },
   "source": [
    "Incorrectly detected faces using the HAAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyYveNZRKYtO"
   },
   "outputs": [],
   "source": [
    "show_face(haar_undetected_images[-5:],scale=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4h79HlhK_ln"
   },
   "source": [
    "Downloading the MTCNN model to detect faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsvvL6w2KYz9"
   },
   "outputs": [],
   "source": [
    "!pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDzRYnsFKY3P"
   },
   "outputs": [],
   "source": [
    "from mtcnn.mtcnn import MTCNN\n",
    "mtcnn_det = MTCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0hLDUvmLEwS"
   },
   "source": [
    "Detecting faces using the MTCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGHDQWijLC8C"
   },
   "outputs": [],
   "source": [
    "mtcnn_tst_img = cv2.imread(raw_img_file_names[0])\n",
    "mt_cvt = cv2.cvtColor(mtcnn_tst_img,cv2.COLOR_BGR2RGB)\n",
    "mt_faces = mtcnn_det.detect_faces(mt_cvt)\n",
    "for face in mt_faces:\n",
    "  mt_x, mt_y,mt_w,mt_h = face['box']\n",
    "  cv2.rectangle(mtcnn_tst_img,(mt_x,mt_y),(mt_x + mt_w,mt_y + mt_h),(255,0,0),2)\n",
    "  cv2.putText(mtcnn_tst_img, \"MTCNN\", (int((mt_x+mt_w)*1),mt_y-3),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255))\n",
    "cv2_imshow(mtcnn_tst_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZsR6Iu_LC_C"
   },
   "outputs": [],
   "source": [
    "mtcnn_img_box_df = pd.DataFrame(columns=['x','y','w','h','Total_Faces','Image_Name'])\n",
    "mtcnn_img_box_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgaZMyGNLDB4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mtcnn_undetected_images = []\n",
    "mtcnn_detected_images = []\n",
    "for imgs, fnames in tqdm(zip(img_list,raw_img_file_names)):\n",
    "  cvt_img = cv2.cvtColor(imgs,cv2.COLOR_BGR2RGB)\n",
    "  faces = mtcnn_det.detect_faces(cvt_img)\n",
    "  if len(faces) == 0:\n",
    "    mtcnn_undetected_images.append(fnames)\n",
    "    temp_dict = {'x':0, \n",
    "                 'y':0, \n",
    "                 'w':-1,\n",
    "                 'h':-1, \n",
    "                 'Total_Faces':0,\n",
    "                 'Image_Name':fnames} \n",
    "  else:\n",
    "    mtcnn_detected_images.append(fnames)\n",
    "    for face in faces:\n",
    "      temp_dict = {'x':face['box'][0], \n",
    "                  'y':face['box'][1], \n",
    "                  'w':face['box'][2],\n",
    "                  'h':face['box'][3], \n",
    "                  'Total_Faces':len(faces),\n",
    "                  'Image_Name':fnames} \n",
    "      mtcnn_img_box_df = mtcnn_img_box_df.append(temp_dict,ignore_index=True)\n",
    "\n",
    "display(Markdown(f\"#### Detected faces for {len(mtcnn_detected_images)} images\"))\n",
    "display(Markdown(f\"#### Failed to detect faces for {len(mtcnn_undetected_images)} images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eS7YtR6pLDN7"
   },
   "outputs": [],
   "source": [
    "mtcnn_img_box_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQVL7W70LDQ-"
   },
   "outputs": [],
   "source": [
    "display(mtcnn_img_box_df[mtcnn_img_box_df['Total_Faces'] > 1])\n",
    "display(Markdown(f\"#### Number of images with more than 1 face detected : {len(mtcnn_img_box_df[mtcnn_img_box_df['Total_Faces'] > 1])}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PInGrWQyLRxF"
   },
   "source": [
    "Viewing samples of correctly and incorrectly detected faces using the MTCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feYYtjTOLDUi"
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"### (1) Correctly detected 1 face\"))\n",
    "test_bb(mtcnn_img_box_df,\"training_images/real_00115.jpg\",title=\"MTCNN\")\n",
    "display(Markdown(\"### (2) Correctly detected one face and incorrectly the other one\"))\n",
    "test_bb(mtcnn_img_box_df,\"training_images/real_00699.jpg\",title=\"MTCNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUQKeM-LLV_a"
   },
   "source": [
    "Incorrectly detected faces using the MTCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wh2LJweLDXP"
   },
   "outputs": [],
   "source": [
    "show_face(mtcnn_undetected_images,scale=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLJPbQk9LcuC"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "There are about 5 images where a face could not be detected due to:\n",
    "\n",
    "Face partailly covered\n",
    "Face zoomed such that it crops part of the face\n",
    "Poor illumination or partially lit surfaces on the face\n",
    "\n",
    "The MTCNN has detected face in images where,\n",
    "\n",
    "Tilted face\n",
    "\n",
    "Side face\n",
    "\n",
    "**Displaying all the faces that were not detected by either HAAR or MTCNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJPrzCUNLYPM"
   },
   "outputs": [],
   "source": [
    "haar_set = set(haar_undetected_images)\n",
    "mtcnn_set = set(mtcnn_undetected_images)\n",
    "show_face(haar_set.intersection(mtcnn_set),0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvvToh4HLoyl"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "When compared performance-wise, MTCNN model did a better job by detecting mostly all faces except for 5 images\n",
    "MTCNN is time consuming compared to HAAR model. This model cannot be used for real-time applications where faces need to be detected fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2yFT9WwLwuQ"
   },
   "source": [
    "**PART C:**\n",
    "\n",
    "**DOMAIN: Face Recognition**\n",
    "\n",
    "• CONTEXT: Company X intends to build a face identification model to recognise human faces.\n",
    "\n",
    "• DATA DESCRIPTION: The dataset comprises of images and its mask where there is a human face.\n",
    "\n",
    "• PROJECT OBJECTIVE: Face Aligned Face Dataset from Pinterest. This dataset contains 10,770 images for 100 people. All images are taken from 'Pinterest' and aligned using dlib library. Some data samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9_-rUJ0fB-P"
   },
   "outputs": [],
   "source": [
    "# Setting the current working directory\n",
    "import os;\n",
    "os.chdir('/content/drive/MyDrive/computervision/FACE_RECOG PROJECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNHpeAlwfWWT"
   },
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZGQgq4nfCBO"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5_Qco2vfCEv"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, sklearn, re, random\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow, cv2\n",
    "%matplotlib inline\n",
    "\n",
    "# Extract content from zipfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Model\n",
    "from tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Encode, standardize and PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set random_state\n",
    "random_state = 2020\n",
    "\n",
    "# Suppress warnings, if any\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# Print versions\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Scikit-learn version: {sklearn.__version__}')\n",
    "print(f'Tensorflow version: {tensorflow.__version__}')\n",
    "print(f'CV version: {cv2.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTCclP3lfhZC"
   },
   "source": [
    "**Extract the zip file**\n",
    "\n",
    "Extract Aligned Face Dataset from PINS.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AeRm-Fvfgf1"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMe_raTHjx5G"
   },
   "outputs": [],
   "source": [
    "Project_path = '/content/drive/MyDrive/computervision/FACE_RECOG PROJECT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4JY2KVpfCJI"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# specifying the zip file name\n",
    "file_name = Project_path + 'PINS.ZIP'\n",
    "  \n",
    "# opening the zip file in READ mode\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "    # printing all the contents of the zip file\n",
    "    # zip.printdir()\n",
    "  \n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    zip.extractall()\n",
    "    print('Done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxQaIm23gHhp"
   },
   "source": [
    "**Function to load images**\n",
    "\n",
    "Define a function to load the images from the extracted folder and map each image with person id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9aenSzxfCME"
   },
   "outputs": [],
   "source": [
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # print(base, name, file)\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "    \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    exts = []\n",
    "    for i in os.listdir(path):\n",
    "        for f in os.listdir(os.path.join(path, i)):\n",
    "            # Check file extension. Allow only jpg/jpeg' files.\n",
    "            ext = os.path.splitext(f)[1]\n",
    "            if ext == '.jpg' or ext == '.jpeg':\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "                exts.append(ext)\n",
    "    return np.array(metadata), exts\n",
    "\n",
    "metadata, exts = load_metadata('PINS')\n",
    "labels = np.array([meta.name for meta in metadata])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5gSKF9PgOEo"
   },
   "source": [
    "**Define function to load image**\n",
    "\n",
    "Define a function to load image from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9o_kc-lfCPS"
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    return img[...,::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sKWmJnfgSwR"
   },
   "source": [
    "**Load a sample image**\n",
    "\n",
    "Load one image using the function \"load_image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9Bi_cs-fCTN"
   },
   "outputs": [],
   "source": [
    "n = np.random.randint(1, len(metadata))\n",
    "img_path = metadata[n].image_path()\n",
    "img = load_image(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjuzpELCfCW5"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 7.2))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "title = labels[n].split('_')[1]\n",
    "ax.set_title(title, fontsize = 20)\n",
    "_ = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snKuXq6TgZoz"
   },
   "source": [
    "**VGG Face model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mA9faSqfCau"
   },
   "outputs": [],
   "source": [
    "def vgg_face():\t\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape = (224, 224, 3)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation = 'relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation = 'relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation = 'relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation = 'relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "    \n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation = 'relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation = 'relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides =(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation = 'relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation = 'relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    \n",
    "    model.add(Convolution2D(4096, (7, 7), activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(4096, (1, 1), activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(2622, (1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoXvViqwgiJa"
   },
   "source": [
    "**Load the model**\n",
    "\n",
    "Load the model defined above\n",
    "\n",
    "Then load the given weight file named \"VGG_FACE_WEIGHTS.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iKtUA7du_-U"
   },
   "outputs": [],
   "source": [
    "weights_file = '/content/drive/MyDrive/computervision/FACE_RECOG PROJECT/VGG_FACE_WEIGHTS.H5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAi-hQC3fChq"
   },
   "outputs": [],
   "source": [
    "model = vgg_face()\n",
    "model.load_weights('VGG_FACE_WEIGHTS.H5')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXQv540thSFv"
   },
   "source": [
    "**Get vgg_face_descriptor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "638uIm90fClI"
   },
   "outputs": [],
   "source": [
    "vgg_face_descriptor = Model(inputs = model.layers[0].input, outputs = model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2nVBEVhhZcr"
   },
   "source": [
    "**Generate embeddings for each image in the dataset**\n",
    "\n",
    "Given below is an example to load the first image in the metadata and get its embedding vector from the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_F5amppZfCpg"
   },
   "outputs": [],
   "source": [
    "# Get embedding vector for first image in the metadata using the pre-trained model\n",
    "\n",
    "img_path = metadata[0].image_path()\n",
    "img = load_image(img_path)\n",
    "\n",
    "# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0, 1]\n",
    "img = (img / 255.).astype(np.float32)\n",
    "\n",
    "img = cv2.resize(img, dsize = (224, 224))\n",
    "print(img.shape)\n",
    "\n",
    "# Obtain embedding vector for an image\n",
    "# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape\n",
    "embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis = 0))[0]\n",
    "print(embedding_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNBx00m1houC"
   },
   "source": [
    "**Generate embeddings for all images**\n",
    "\n",
    "Write code to iterate through metadata and create embeddings for each image using vgg_face_descriptor.predict() and store in a list with name embeddings\n",
    "\n",
    "If there is any error in reading any image in the dataset, fill the emebdding vector of that image with 2622-zeroes as the final embedding from the model is of length 2622."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeQyBbSSfCtT"
   },
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "embeddings = np.zeros((metadata.shape[0], 2622))\n",
    "for i, meta in tqdm(enumerate(metadata)):\n",
    "  try:\n",
    "    image = load_image(str(meta))\n",
    "    image = (image/255.).astype(np.float32)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    embeddings[i] = vgg_face_descriptor.predict(np.expand_dims(image, axis = 0))[0]\n",
    "  except:\n",
    "    embeddings[i] = np.zeros(2622)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqFjIhyOh1K4"
   },
   "source": [
    "**Function to calculate distance between given 2 pairs of images.**\n",
    "\n",
    "Consider distance metric as \"Squared L2 distance\"\n",
    "\n",
    "Squared l2 distance between 2 points (x1, y1) and (x2, y2) = (x1-x2)^2 + (y1-y2)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jq-T4Am5fCxQ"
   },
   "outputs": [],
   "source": [
    "def distance(emb1, emb2):\n",
    "    return np.sum(np.square(emb1 - emb2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIbqGxFsh-nI"
   },
   "source": [
    "Plot images and get distance between the pairs given below\n",
    "\n",
    "*   2, 3 and 2, 180\n",
    "*   30, 31 and 30, 100\n",
    "*   70, 72 and 70, 115\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEc7LFdMfC1T"
   },
   "outputs": [],
   "source": [
    "def show_pair(idx1, idx2):\n",
    "    plt.figure(figsize = (8, 3))\n",
    "    plt.suptitle(f'Distance = {distance(embeddings[idx1], embeddings[idx2]):.2f}')\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(load_image(metadata[idx1].image_path()))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(load_image(metadata[idx2].image_path()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUVovXpYfC5C"
   },
   "outputs": [],
   "source": [
    "show_pair(2, 3)\n",
    "show_pair(2, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfX0jgAafC9N"
   },
   "outputs": [],
   "source": [
    "show_pair(30, 31)\n",
    "show_pair(30, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m74oKT6nfDAi"
   },
   "outputs": [],
   "source": [
    "show_pair(70, 72)\n",
    "show_pair(70, 115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESmRhvPuiXzL"
   },
   "source": [
    "**Create train and test sets**\n",
    "\n",
    "* Create X_train, X_test and y_train, y_test\n",
    "* Use train_idx to seperate out training features and labels\n",
    "* Use test_idx to seperate out testing features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdEslQAsfDEN"
   },
   "outputs": [],
   "source": [
    "train_idx = np.arange(metadata.shape[0]) % 9 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 9 == 0\n",
    "\n",
    "# Features\n",
    "X_train = np.array(embeddings)[train_idx]\n",
    "X_test = np.array(embeddings)[test_idx]\n",
    "\n",
    "# Labels\n",
    "y_train = np.array([meta.name for meta in metadata[train_idx]])\n",
    "y_test = np.array([meta.name for meta in metadata[test_idx]])\n",
    "\n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq-_cIz_ifFB"
   },
   "source": [
    "**Encode the Labels**\n",
    "* Encode the labels\n",
    "* Use LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrVpKcwmfDH-"
   },
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "en = LabelEncoder()\n",
    "y_train = en.fit_transform(y_train)\n",
    "y_test = en.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUPOE6UoikcS"
   },
   "source": [
    "**Standardize the feature values**\n",
    "\n",
    "Scale the features using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f408OjSNfDLV"
   },
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZbYSx1GipHB"
   },
   "source": [
    "**Reduce dimensions using PCA**\n",
    "\n",
    "Reduce feature dimensions using Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRp8bb2bfDO9"
   },
   "outputs": [],
   "source": [
    "# Covariance matrix\n",
    "cov_matrix = np.cov(X_train_sc.T)\n",
    "\n",
    "# Eigen values and vector\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Cumulative variance explained\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i /tot) * 100 for i in sorted(eig_vals, reverse = True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "print('Cumulative Variance Explained', cum_var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SDCbPvNfDSZ"
   },
   "outputs": [],
   "source": [
    "# Get index where cumulative variance explained is > threshold\n",
    "thres = 95\n",
    "res = list(filter(lambda i: i > thres, cum_var_exp))[0]\n",
    "index = (cum_var_exp.tolist().index(res))\n",
    "print(f'Index of element just greater than {thres}: {str(index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YniLom9diuim"
   },
   "outputs": [],
   "source": [
    "# Ploting \n",
    "plt.figure(figsize = (15 , 7.2))\n",
    "plt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
    "plt.step(range(1, eig_vals.size + 1), cum_var_exp, where = 'mid', label = 'Cumulative explained variance')\n",
    "plt.axhline(y = thres, color = 'r', linestyle = '--')\n",
    "plt.axvline(x = index, color = 'r', linestyle = '--')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fpX30d_iunv"
   },
   "outputs": [],
   "source": [
    "# Reducing the dimensions\n",
    "pca = PCA(n_components = index, random_state = random_state, svd_solver = 'full', whiten = True)\n",
    "pca.fit(X_train_sc)\n",
    "X_train_pca = pca.transform(X_train_sc)\n",
    "X_test_pca = pca.transform(X_test_sc)\n",
    "display(X_train_pca.shape, X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au5idoIPizwN"
   },
   "source": [
    "**Build a Classifier**\n",
    "\n",
    "* Use SVM Classifier to predict the person in the given image\n",
    "* Fit the classifier and print the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxn1W2r4iur_"
   },
   "outputs": [],
   "source": [
    "svc_pca = SVC(C = 1, gamma = 0.001, kernel = 'rbf', class_weight = 'balanced', random_state = random_state)\n",
    "svc_pca.fit(X_train_pca, y_train)\n",
    "print('SVC accuracy for train set: {0:.3f}'.format(svc_pca.score(X_train_pca, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2sR7fR8iuwU"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = svc_pca.predict(X_test_pca)\n",
    "\n",
    "# Accuracy Score\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test, y_pred).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6SILeDmiu0r"
   },
   "outputs": [],
   "source": [
    "names = [name.split('_')[1].title().strip() for name in labels]\n",
    "\n",
    "# Classification Report\n",
    "print('Classification Report: \\n{}'.format(classification_report(y_test, y_pred, target_names = np.unique(names))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bookJ4vi9xT"
   },
   "source": [
    "**Test results**\n",
    "\n",
    "* Take 10th image from test set and plot the image\n",
    "* Report to which person(folder name in dataset) the image belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IR6tihQOiu4J"
   },
   "outputs": [],
   "source": [
    "def sample_img_plot(sample_idx):\n",
    "  # Load image for sample_idx from test data\n",
    "  sample_img = load_image(metadata[test_idx][sample_idx].image_path())\n",
    "  # Get actual name\n",
    "  actual_name = metadata[test_idx][sample_idx].name.split('_')[-1].title().strip()\n",
    "  # Normalizing pixel values\n",
    "  sample_img = (sample_img/255.).astype(np.float32)\n",
    "  # Resize\n",
    "  sample_img = cv2.resize(sample_img, (224, 224))\n",
    "\n",
    "  # Obtain embedding vector for sample image\n",
    "  embedding = vgg_face_descriptor.predict(np.expand_dims(sample_img, axis = 0))[0]\n",
    "  # Scaled the vector and reshape\n",
    "  embedding_scaled = sc.transform(embedding.reshape(1, -1))\n",
    "  # Predict\n",
    "  sample_pred = svc_pca.predict(pca.transform(embedding_scaled))\n",
    "  # Transform back\n",
    "  pred_name = en.inverse_transform(sample_pred)[0].split('_')[-1].title().strip()\n",
    "  return sample_img, actual_name, pred_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSJogCYGiu7s"
   },
   "outputs": [],
   "source": [
    "# Plot for 10th image in test data\n",
    "sample_img, actual_name, pred_name = sample_img_plot(10)\n",
    "fig = plt.figure(figsize = (15, 7.2))\n",
    "plt.axis('off')\n",
    "plt.imshow(sample_img)\n",
    "plt.title(f\"A: {actual_name} \\n P: {pred_name}\", color = 'green' if actual_name == pred_name else 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vw5ld7FHiu_z"
   },
   "outputs": [],
   "source": [
    "# Random 20 sample images from test data\n",
    "plt.figure(figsize = (15, 15))\n",
    "gs1 = gridspec.GridSpec(5, 4)\n",
    "gs1.update(wspace = 0, hspace = 0.3) \n",
    "\n",
    "for i in range(20):\n",
    "    ax1 = plt.subplot(gs1[i])\n",
    "    plt.axis('on')\n",
    "    ax1.set_xticklabels([])\n",
    "    ax1.set_yticklabels([])\n",
    "    ax1.set_aspect('equal')\n",
    "    \n",
    "    sample_img, actual_name, pred_name = sample_img_plot(random.randint(1, 1197))\n",
    "  \n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample_img)\n",
    "  \n",
    "    plt.title(f\"A: {actual_name} \\n P: {pred_name}\", color = 'green' if actual_name == pred_name else 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_qMDYFOjH_B"
   },
   "source": [
    "**Conclusion**\n",
    "\n",
    "* Task here was to recognize (aligned) faces from a dataset containing 10k+ images for 100 people using a pre-trained model on Face Recognition.\n",
    "\n",
    "* VGG model with pre-trained weights was used to generate embeddings for each images in the dataset.\n",
    "* Distance between two pair of images were also calculated and plotted.\n",
    "Since, there were 2,622 features for each image, PCA was used for dimension reduction after standardizing the features.\n",
    "* With an cumulative explained variance of 95%, 347 PCA components were used.\n",
    "Using SVC we predicted the labels for test dataset with an accuracy of more than 96%.\n",
    "* Also compared predicted and actual labels for a given sample image as well as for 20 random images from test dataset."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
